{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### University of Virginia\n",
    "### DS 5110: Big Data Systems\n",
    "\n",
    "### Classification of Wisconsin Breast Cancer Database\n",
    "### Last updated: June 17, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions** \n",
    "\n",
    "In this project, you will work with the Wisconsin Breast Cancer dataset.  You will train a logistic regression model to predict the diagnosis.  First, you will work through this example, **filling in the missing cells.**  Then you will make modifications and run the code, collecting results at the bottom of the notebook.\n",
    "\n",
    "The following experiments should be conducted:\n",
    "1.  Three features were used in the original model.  **Build the model using all features.**\n",
    "Before training the model, apply scaling to the features using the StandardScaler\n",
    "transformer.  Then train the model and compute and show the accuracy and confusion matrix, **measured on the test set.**\n",
    "\n",
    "**Hint**: While the data is in a dataframe, this might be helpful:\n",
    "```\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "```\n",
    "\n",
    "2. Repeat step (1), including an intercept\n",
    "3. Repeat step (1), using randomSplit([0.7, 0.3]) but NO intercept\n",
    "\n",
    "**Total Possible Points: 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param init \n",
    "infile = 'wisc_breast_cancer_w_fields.csv'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Wisc BRCA\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data into dataframe\n",
    "df = spark.read.csv(infile, inferSchema=True, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 PT)** Combine fields *f1*, *f2*, *f3* into a single *features* column using `VectorAssembler`  \n",
    "Name the resulting dataframe *transformed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"f1\", \"f2\", \"f3\"], outputCol=\"features\") \n",
    "transformed = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the *diagnosis* and *features* fields for modeling.  \n",
    "We will do the remaining steps with RDDs, so we convert to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRdd = transformed.select(\"diagnosis\", \"features\").rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', DenseVector([17.99, 10.38, 122.8])),\n",
       " ('M', DenseVector([20.57, 17.77, 132.9]))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at some data\n",
    "dataRdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map label to binary values, then convert to LabeledPoint\n",
    "lp = dataRdd.map(lambda row:(1 if row[0]=='M' else 0, Vectors.dense(row[1])))    \\\n",
    "                    .map(lambda row: LabeledPoint(row[0], row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [17.99,10.38,122.8]),\n",
       " LabeledPoint(1.0, [20.57,17.77,132.9])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at some data\n",
    "lp.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 PT)** Split data approximately into training (60%) and test (40%) using `seed=314`  \n",
    "The RDDs that are output from the splitting should be named *training*, *test*, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = lp.randomSplit([0.6, 0.4], seed=314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(356, 213, 569)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count records in datasets\n",
    "(training.count(), test.count(), lp.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6256590509666081, 0.37434094903339193, 1.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(training.count()/lp.count(), test.count()/lp.count(), lp.count()/lp.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 PT)** Train model `LogisticRegressionWithLBFGS`, naming it *model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionWithLBFGS.train(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 PT)**  Evaluate the model by computing the accuracy on the **test data**. Print the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy (test): 0.8732394366197183\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on test data\n",
    "labelsAndPreds_te = test.map(lambda p: (p.label, model.predict(p.features)))\n",
    "accuracy_te = 1.0 * labelsAndPreds_te.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n",
    "print('model accuracy (test): {}'.format(accuracy_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTIONS**  \n",
    " For parts 1-3, compute and show for the test set: (1) accuracy (2) confusion matrix.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter solution for Part 1 (2 POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy (test): 0.9624413145539906\n",
      "Confusion Matrix:\n",
      "[[135.   4.]\n",
      " [  4.  70.]]\n"
     ]
    }
   ],
   "source": [
    "# use all of the fields as features\n",
    "assembler = VectorAssembler(inputCols=[i for i in df.columns if i[0]=='f'], outputCol=\"features\") \n",
    "transformed = assembler.transform(df)\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(transformed)\n",
    "scaledData = scalerModel.transform(transformed)\n",
    "\n",
    "# convert to RDD\n",
    "dataRdd = scaledData.select(\"diagnosis\",\"scaledFeatures\").rdd.map(tuple)\n",
    "\n",
    "# map label to binary values, then create LabeledPoints\n",
    "lp = dataRdd.map(lambda row: LabeledPoint(1 if row[0]=='M' else 0, Vectors.dense(row[1])))\n",
    "\n",
    "# Split data approximately into training (60%) and test (40%)\n",
    "training, test = lp.randomSplit([0.6, 0.4], seed=314)\n",
    "\n",
    "# Build the model\n",
    "model = LogisticRegressionWithLBFGS.train(training)\n",
    "\n",
    "# Evaluating the model on test data\n",
    "\n",
    "# make sure predictions are floats\n",
    "labelsAndPreds_te = test.map(lambda p: (p.label, float(model.predict(p.features))))\n",
    "accuracy_te = 1.0 * labelsAndPreds_te.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n",
    "print('model accuracy (test): {}'.format(accuracy_te))\n",
    "\n",
    "metrics = MulticlassMetrics(labelsAndPreds_te)\n",
    "labelsAndPreds_te.take(3)\n",
    "print(\"Confusion Matrix:\\n{}\".format(metrics.confusionMatrix().toArray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter solution for Part 2  (2 POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy (test): 0.9671361502347418\n",
      "Confusion Matrix:\n",
      "[[135.   3.]\n",
      " [  4.  71.]]\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=[i for i in df.columns if i[0]=='f'], outputCol=\"features\") \n",
    "transformed = assembler.transform(df)\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(transformed)\n",
    "scaledData = scalerModel.transform(transformed)\n",
    "\n",
    "dataRdd = scaledData.select(\"diagnosis\",\"scaledFeatures\").rdd.map(tuple)\n",
    "\n",
    "lp = dataRdd.map(lambda row: LabeledPoint(1 if row[0]=='M' else 0, Vectors.dense(row[1])))\n",
    "\n",
    "training, test = lp.randomSplit([0.6, 0.4], seed=314)\n",
    "\n",
    "model = LogisticRegressionWithLBFGS.train(training, intercept=True)\n",
    "\n",
    "labelsAndPreds_te = test.map(lambda p: (p.label, float(model.predict(p.features))))\n",
    "accuracy_te = 1.0 * labelsAndPreds_te.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n",
    "print('model accuracy (test): {}'.format(accuracy_te))\n",
    "\n",
    "metrics = MulticlassMetrics(labelsAndPreds_te)\n",
    "labelsAndPreds_te.take(3)\n",
    "print(\"Confusion Matrix:\\n{}\".format(metrics.confusionMatrix().toArray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter solution for Part 3 (2 POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy (test): 0.9433962264150944\n",
      "Confusion Matrix:\n",
      "[[98.  6.]\n",
      " [ 3. 52.]]\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=[i for i in df.columns if i[0]=='f'], outputCol=\"features\") \n",
    "transformed = assembler.transform(df)\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(transformed)\n",
    "scaledData = scalerModel.transform(transformed)\n",
    "\n",
    "dataRdd = scaledData.select(\"diagnosis\",\"scaledFeatures\").rdd.map(tuple)\n",
    "\n",
    "lp = dataRdd.map(lambda row: LabeledPoint(1 if row[0]=='M' else 0, Vectors.dense(row[1])))\n",
    "\n",
    "training, test = lp.randomSplit([0.7, 0.3], seed=314)\n",
    "\n",
    "model = LogisticRegressionWithLBFGS.train(training)\n",
    "\n",
    "labelsAndPreds_te = test.map(lambda p: (p.label, float(model.predict(p.features))))\n",
    "accuracy_te = 1.0 * labelsAndPreds_te.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n",
    "print('model accuracy (test): {}'.format(accuracy_te))\n",
    "\n",
    "metrics = MulticlassMetrics(labelsAndPreds_te)\n",
    "labelsAndPreds_te.take(3)\n",
    "print(\"Confusion Matrix:\\n{}\".format(metrics.confusionMatrix().toArray()))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
